#PPO config for snake

# Environment
env: snake-v0
num_workers: 16
num_gpus: 1 
num_cpus_per_worker: 1
disable_env_checking: true
framework: torch
use_eager_tracing: true
clip_actions : true
normalize_actions: true
observation_filter: MeanStdFilter 
optimizer:
    type: Adam

#ppo config 
kl_coeff: 0.2
batch_mode: truncate_episodes

num_sgd_iter: 1
clip_param: 0.2
sgd_minibatch_size: 2000
train_batch_size: 2000
gamma: 0.99
lambda: 0.95
vf_loss_coeff: 0.5
entropy_coeff: 0.01
use_gae: True
kl_target: 0.01
use_critic: true
recreate_failed_workers: true
reuse_actors: True

#the model has two dense layers with 64 neurons each
model:
    fcnet_hiddens: [64,64]
    fcnet_activation: swish
    vf_share_layers: false 
    free_log_std: true
